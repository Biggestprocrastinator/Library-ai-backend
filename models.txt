{
  "ok": true,
  "version": "2024-05-31",
  "total_count": 28,
  "limit": 100,
  "resources": [
    {
      "model_id": "cross-encoder/ms-marco-minilm-l-12-v2",
      "label": "ms-marco-minilm-l-12-v2",
      "provider": "cross-encoder",
      "source": "cross-encoder",
      "indemnity": "NON_IBM",
      "functions": [
        {
          "id": "rerank"
        }
      ],
      "short_description": "Used for Information Retrieval: Encode and sort a query will all possible passages.",
      "long_description": "The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order.",
      "input_tier": "class_11",
      "output_tier": "class_11",
      "number_params": "33.4m",
      "model_limits": {
        "max_sequence_length": 512
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-09-17"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 16,
        "balance_mode": 2
      }
    },
    {
      "model_id": "ibm/granite-3-1-8b-base",
      "label": "granite-3-1-8b-base",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "base_foundation_model_deployable"
        },
        {
          "id": "lora_fine_tune_trainable"
        }
      ],
      "short_description": "Granite 3.1 8b base is a pre-trained autoregressive foundation model with a context length of 128k intended for tuning.",
      "long_description": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
      "terms_url": "https://www.ibm.com/support/customer/csol/terms/?id=i126-6883&lc=en",
      "input_tier": "",
      "output_tier": "",
      "number_params": "8b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation",
        "function_calling",
        "code-generation",
        "code-explanation",
        "code-fixing"
      ],
      "tasks": [
        {
          "id": "question_answering"
        },
        {
          "id": "summarization"
        },
        {
          "id": "retrieval_augmented_generation"
        },
        {
          "id": "classification"
        },
        {
          "id": "generation"
        },
        {
          "id": "code"
        },
        {
          "id": "extraction"
        },
        {
          "id": "translation"
        },
        {
          "id": "function_calling"
        },
        {
          "id": "code-generation"
        },
        {
          "id": "code-explanation"
        },
        {
          "id": "code-fixing"
        }
      ],
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2025-04-16"
        }
      ],
      "versions": [
        {
          "version": "3.1.0",
          "available_date": "2024-12-06"
        }
      ],
      "lora_fine_tuning_parameters": {
        "num_epochs": {
          "default": 10,
          "min": 1,
          "max": 50
        },
        "verbalizer": {
          "default": "### Input: {{input}} \n\n### Response: {{output}}"
        },
        "batch_size": {
          "default": 5,
          "min": 1,
          "max": 16
        },
        "accumulate_steps": {
          "default": 1,
          "min": 1,
          "max": 128
        },
        "learning_rate": {
          "default": 0.00001,
          "min": 0.00001,
          "max": 0.5
        },
        "max_seq_length": {
          "default": 4096,
          "min": 1,
          "max": 8192
        },
        "tokenizer": {
          "default": "ibm/granite-3-1-8b-base"
        },
        "response_template": {
          "default": "\n### Response:"
        },
        "num_gpus": {
          "default": 1
        },
        "peft_parameters": {
          "type": {
            "supported": [
              "lora"
            ],
            "default": "lora"
          },
          "rank": {
            "supported": [
              8,
              16,
              32,
              64,
              128,
              256
            ],
            "default": 32
          },
          "target_modules": {
            "supported": [
              "all-linear",
              "o_proj",
              "v_proj",
              "k_proj",
              "q_proj",
              "up_proj",
              "down_proj",
              "gate_proj"
            ],
            "default": [
              "all-linear"
            ]
          },
          "lora_alpha": {
            "default": 32,
            "min": 0,
            "max": 999999
          },
          "lora_dropout": {
            "default": 0.05,
            "min": 0,
            "max": 1
          }
        },
        "gradient_checkpointing": {
          "default": true
        }
      },
      "architecture_type": "granite",
      "curated_model_info": {
        "base_model_id": "ibm/granite-3-1-8b-base",
        "lora_hardware_spec": "WXaaS-S-Lora",
        "additional_params": "VLLM_USE_V1=1,HOME=/home/vllm,OMP_NUM_THREADS=2,MAX_SEQUENCE_LENGTH=131072,MAX_LORAS=8,MAX_CPU_LORAS=10,MAX_LORA_RANK=256,DTYPE=bfloat16",
        "gpu_requirements": {
          "min_count": {
            "a100": 1,
            "h100": 1
          }
        }
      },
      "deployment_parameters": [
        {
          "name": "enable_lora",
          "display_name": "Enable Lora",
          "default": false,
          "type": "boolean"
        }
      ]
    },
    {
      "model_id": "ibm/granite-3-2-8b-instruct",
      "label": "granite-3-2-8b-instruct",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "Granite 3.2 8b Instruct is a text-only model capable of reasoning which can be enabled or disabled by the user to use the right capability for the right use case.",
      "long_description": "Granite 3.2 8b Instruct is a text-only model capable of reasoning which can be enabled or disabled by the user to use the right capability for the right use case.",
      "terms_url": "https://www.ibm.com/support/customer/csol/terms/?id=i126-6883",
      "input_tier": "class_12",
      "output_tier": "class_12",
      "number_params": "8b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation",
        "function_calling"
      ],
      "tasks": [
        {
          "id": "question_answering"
        },
        {
          "id": "summarization"
        },
        {
          "id": "retrieval_augmented_generation"
        },
        {
          "id": "classification"
        },
        {
          "id": "generation"
        },
        {
          "id": "code"
        },
        {
          "id": "extraction"
        },
        {
          "id": "translation"
        },
        {
          "id": "function_calling",
          "ratings": {
            "quality": 3
          }
        }
      ],
      "model_limits": {
        "max_sequence_length": 131072
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2025-02-26"
        },
        {
          "id": "deprecated",
          "start_date": "2025-11-24",
          "alternative_model_ids": [
            "ibm/granite-4-h-small"
          ]
        },
        {
          "id": "withdrawn",
          "start_date": "2026-02-22",
          "alternative_model_ids": [
            "ibm/granite-4-h-small"
          ]
        }
      ],
      "versions": [
        {
          "version": "1.0.0",
          "available_date": "2025-02-26"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 16,
        "balance_mode": 2
      }
    },
    {
      "model_id": "ibm/granite-3-3-8b-instruct",
      "label": "granite-3-3-8b-instruct",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "autoai_rag"
        },
        {
          "id": "multilingual"
        },
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "Granite-3.3-8b-Instruct is an IBM-trained, dense decoder-only models, which is particularly well-suited for generative tasks.",
      "long_description": "Granite-3.3-8b-Instruct is designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. It employs a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
      "terms_url": "https://www.ibm.com/support/customer/csol/terms/?id=i126-6883&lc=en",
      "input_tier": "class_12",
      "output_tier": "class_12",
      "number_params": "8b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation",
        "function_calling"
      ],
      "tasks": [
        {
          "id": "question_answering"
        },
        {
          "id": "summarization"
        },
        {
          "id": "retrieval_augmented_generation"
        },
        {
          "id": "classification"
        },
        {
          "id": "generation"
        },
        {
          "id": "code"
        },
        {
          "id": "extraction"
        },
        {
          "id": "translation"
        },
        {
          "id": "function_calling"
        }
      ],
      "model_limits": {
        "max_sequence_length": 131072
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2025-04-16"
        },
        {
          "id": "deprecated",
          "start_date": "2025-11-24",
          "alternative_model_ids": [
            "ibm/granite-4-h-small"
          ]
        },
        {
          "id": "withdrawn",
          "start_date": "2026-02-22",
          "alternative_model_ids": [
            "ibm/granite-4-h-small"
          ]
        }
      ],
      "versions": [
        {
          "version": "3.3.0",
          "available_date": "2025-04-16"
        }
      ],
      "supported_languages": [
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th"
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 8,
        "balance_mode": 2
      }
    },
    {
      "model_id": "ibm/granite-3-8b-instruct",
      "label": "granite-3-8b-instruct",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "autoai_rag"
        },
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
      "long_description": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
      "terms_url": "https://www.ibm.com/support/customer/csol/terms/?id=i126-6883",
      "input_tier": "class_12",
      "output_tier": "class_12",
      "number_params": "8b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation",
        "function_calling",
        "code-generation",
        "code-explanation",
        "code-fixing"
      ],
      "tasks": [
        {
          "id": "question_answering",
          "benchmarks": [
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "FinQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 5
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 54.23
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 86.97
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 85.67
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 49.39
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 46.73
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 82.61
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 83.1
                }
              ],
              "tags": [
                "Knowledge"
              ]
            }
          ]
        },
        {
          "id": "summarization",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.",
              "language": "English",
              "dataset": {
                "name": "Summarization"
              },
              "prompt": {},
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 31.41
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset that summarizes US Congressional and California state bills from 1993â€“2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "English",
              "dataset": {
                "name": "BillSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 28.96
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "English",
              "dataset": {
                "name": "TLDR17"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 8.04
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "French",
              "dataset": {
                "name": "MLSUM"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 12.42
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "German",
              "dataset": {
                "name": "MLSUM"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 11.58
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Arabic",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 4.15
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "French",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 13.44
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Japanese",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 17.29
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Korean",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 11.04
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Portuguese",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 16.3
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Spanish",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 12.39
                }
              ],
              "tags": [
                "Summarization"
              ]
            }
          ]
        },
        {
          "id": "retrieval_augmented_generation",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.",
              "language": "English",
              "dataset": {
                "name": "RAG"
              },
              "prompt": {},
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 34.82
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "CLAP NQ"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 51.26
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "French",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 81.77
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "German",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 76.79
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "Portuguese",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 82.2
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "Spanish",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 78.24
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            }
          ]
        },
        {
          "id": "classification",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.",
              "language": "English",
              "dataset": {
                "name": "Classification"
              },
              "prompt": {},
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 57.84
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "20 Newsgroups"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 53.27
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "BBQ"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 79.55
                }
              ],
              "tags": [
                "Safety & Bias"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "CFPB"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 74.39
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "HellaSwag"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 47
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "LegalBench"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 52.18
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "MMLU-Pro"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 42.86
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "OpenBookQA"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 88
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 80.47
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 91.41
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 86.72
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 82.81
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 79.69
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 89.84
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 90.62
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 63.25
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 69.83
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 75.86
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 83.68
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 71.05
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 66.67
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 63.06
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 67.51
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 82.99
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 81.97
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 86.07
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 73.64
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 81.48
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 82.5
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 41.72
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 52.39
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 50.74
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 42.62
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 44.3
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 53.02
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 52.95
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 41.51
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 52.61
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 51.18
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 43.45
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 45.3
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 53.22
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 53
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 41.41
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 61.72
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 61.72
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 67.19
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 71.09
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 71.88
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 72.66
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 73.44
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XWinograd"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 77.34
                }
              ],
              "tags": [
                "Reasoning"
              ]
            }
          ]
        },
        {
          "id": "generation",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.",
              "language": "English",
              "dataset": {
                "name": "Generation"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "SacreBLEU",
                  "value": 11.18
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.",
              "language": "English",
              "dataset": {
                "name": "Arena-Hard"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Win rate",
                  "value": 11.06
                }
              ],
              "tags": [
                "Chatbot Ability"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.",
              "language": "English",
              "dataset": {
                "name": "AttaQ 500"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Safety",
                  "value": 93.33
                }
              ],
              "tags": [
                "Safety & Bias"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "French",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 24.66
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "German",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 28.95
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "Portuguese",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 43.88
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "Spanish",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 48.5
                }
              ],
              "tags": [
                "Generation"
              ]
            }
          ]
        },
        {
          "id": "code"
        },
        {
          "id": "extraction",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.",
              "language": "English",
              "dataset": {
                "name": "Extraction"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 40.31
                }
              ],
              "tags": [
                "Extraction"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "Universal NER"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 44.23
                }
              ],
              "tags": [
                "Extraction"
              ]
            }
          ]
        },
        {
          "id": "translation",
          "benchmarks": [
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.",
              "language": "English",
              "dataset": {
                "name": "FLoRes-101"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "SacreBLEU",
                  "value": 32
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "French",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 91.41
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "German",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 88.82
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Japanese",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 75.29
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Korean",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 29.53
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Portuguese",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 90.47
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Spanish",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 90.24
                }
              ],
              "tags": [
                "Translation"
              ]
            }
          ]
        },
        {
          "id": "function_calling",
          "ratings": {
            "quality": 3
          }
        },
        {
          "id": "code-generation",
          "benchmarks": [
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.646
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to solve entry-level Python problems using a dataset with 974 crowd-sourced problems and solutions. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "MBPP"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.496
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "C++",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.518
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Go",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.354
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Java",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.543
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "JavaScript",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.421
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.713
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Rust",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.311
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "",
              "language": "Python",
              "dataset": {
                "name": "livecodebench_codeexecution"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.382
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "",
              "language": "Python",
              "dataset": {
                "name": "livecodebench_codegeneration"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.155
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "",
              "language": "Python",
              "dataset": {
                "name": "livecodebench_testoutputprediction"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.287
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to solve entry-level Python problems using a dataset with 974 crowd-sourced problems and solutions. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "MBPP"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.472
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Expands on the MBPP dataset with more Python programming problems and more comprehensive test cases. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "MBPP+"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.45
                }
              ],
              "tags": [
                "Code generation"
              ]
            }
          ]
        },
        {
          "id": "code-explanation",
          "benchmarks": [
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.572
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "C++",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.494
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Go",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.341
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Java",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.628
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "JavaScript",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.488
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.64
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Rust",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.305
                }
              ],
              "tags": [
                "Code explanation"
              ]
            }
          ]
        },
        {
          "id": "code-fixing",
          "benchmarks": [
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.658
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "C++",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.36
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Go",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.317
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Java",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.396
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "JavaScript",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.402
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.433
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Rust",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.256
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "",
              "language": "Python",
              "dataset": {
                "name": "livecodebench_selfrepair"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.156
                }
              ],
              "tags": [
                "Code fixing"
              ]
            }
          ]
        }
      ],
      "model_limits": {
        "max_sequence_length": 131072
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-10-21"
        },
        {
          "id": "deprecated",
          "start_date": "2025-11-24",
          "alternative_model_ids": [
            "ibm/granite-4-h-small"
          ]
        },
        {
          "id": "withdrawn",
          "start_date": "2026-02-22",
          "alternative_model_ids": [
            "ibm/granite-4-h-small"
          ]
        }
      ],
      "versions": [
        {
          "version": "1.1.0",
          "available_date": "2024-12-13"
        },
        {
          "version": "1.0.0",
          "available_date": "2024-10-21"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 8,
        "balance_mode": 2
      }
    },
    {
      "model_id": "ibm/granite-4-h-small",
      "label": "granite-4-h-small",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "autoai_rag"
        },
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "Granite-4.0-H-Small is a 30B parameter long-context instruct model finetuned from Granite-4.0-H-Small-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets.",
      "long_description": "Granite-4.0-H-Small is a 30B parameter long-context instruct model finetuned from Granite-4.0-H-Small-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets. This model is developed using a diverse set of techniques with a structured chat format, including supervised finetuning, model alignment using reinforcement learning, and model merging. Granite 4.0 instruct models feature improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications.",
      "terms_url": "https://www.ibm.com/support/customer/csol/terms/?id=i126-6883&lc=en",
      "input_tier": "class_18",
      "output_tier": "class_5",
      "number_params": "32b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation",
        "function_calling",
        "code-generation",
        "code-explanation",
        "code-fixing"
      ],
      "tasks": [
        {
          "id": "question_answering"
        },
        {
          "id": "summarization"
        },
        {
          "id": "retrieval_augmented_generation"
        },
        {
          "id": "classification"
        },
        {
          "id": "generation"
        },
        {
          "id": "code"
        },
        {
          "id": "extraction"
        },
        {
          "id": "translation"
        },
        {
          "id": "function_calling"
        }
      ],
      "model_limits": {
        "max_sequence_length": 131072
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2025-10-02"
        }
      ],
      "versions": [
        {
          "version": "4.0.0",
          "available_date": "2025-10-02"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 8,
        "balance_mode": 2
      }
    },
    {
      "model_id": "ibm/granite-8b-code-instruct",
      "label": "granite-8b-code-instruct",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "text_generation"
        }
      ],
      "short_description": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
      "long_description": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
      "terms_url": "https://www.ibm.com/support/customer/csol/terms/?id=i126-6883",
      "input_tier": "class_1",
      "output_tier": "class_1",
      "number_params": "8b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "classification",
        "generation",
        "code",
        "extraction",
        "code-generation",
        "code-explanation",
        "code-fixing"
      ],
      "tasks": [
        {
          "id": "question_answering"
        },
        {
          "id": "summarization"
        },
        {
          "id": "classification"
        },
        {
          "id": "generation"
        },
        {
          "id": "code"
        },
        {
          "id": "extraction"
        },
        {
          "id": "code-generation",
          "benchmarks": [
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "C++",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.482
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Go",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.433
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Java",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.585
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "JavaScript",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.524
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.579
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Rust",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.372
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "C++",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.482
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Go",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.402
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Java",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.573
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "JavaScript",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.549
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.585
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to generate code in Python, C++, Go, Java, JavaScript, and Rust by using 164 code problems from the 'HumanEval' dataset that were translated from Python by humans. Metric represents the pass@1 score.",
              "language": "Rust",
              "dataset": {
                "name": "HumanEvalSynthesize"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.348
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "",
              "language": "Python",
              "dataset": {
                "name": "livecodebench_codeexecution"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.236
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "",
              "language": "Python",
              "dataset": {
                "name": "livecodebench_codegeneration"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.127
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "",
              "language": "Python",
              "dataset": {
                "name": "livecodebench_testoutputprediction"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.208
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to solve entry-level Python problems using a dataset with 974 crowd-sourced problems and solutions. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "MBPP"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.502
                }
              ],
              "tags": [
                "Code generation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Expands on the MBPP dataset with more Python programming problems and more comprehensive test cases. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "MBPP+"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.571
                }
              ],
              "tags": [
                "Code generation"
              ]
            }
          ]
        },
        {
          "id": "code-explanation",
          "benchmarks": [
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "C++",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.439
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Go",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.366
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Java",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.524
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "JavaScript",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.427
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.53
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Rust",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.165
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "C++",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.396
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Go",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.396
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Java",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.476
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "JavaScript",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.39
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.506
                }
              ],
              "tags": [
                "Code explanation"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to explain C++, Go, Java, JavaScript, Python and Rust code by using the 'HumanEval' dataset to first ask the model to explain the solution to a programming problem and then solve the problem given only the previously generated explanation. Metric represents the pass@1 score.",
              "language": "Rust",
              "dataset": {
                "name": "HumanEvalExplain"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.22
                }
              ],
              "tags": [
                "Code explanation"
              ]
            }
          ]
        },
        {
          "id": "code-fixing",
          "benchmarks": [
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "C++",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.39
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Go",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.415
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Java",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.482
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "JavaScript",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.409
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.396
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "academic",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Rust",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.329
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "C++",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.39
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Go",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.421
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Java",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.482
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "JavaScript",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.427
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Python",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.402
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "Evaluates a model's ability to fix coding errors in the C++, Go, Java, JavaScript, Python and Rust using 'HumanEval' dataset with introduced errors and unit tests to help identify problems. Metric represents the pass@1 score.",
              "language": "Rust",
              "dataset": {
                "name": "HumanEvalFix"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.28
                }
              ],
              "tags": [
                "Code fixing"
              ]
            },
            {
              "type": "watsonx.ai",
              "name": "Code",
              "description": "",
              "language": "Python",
              "dataset": {
                "name": "livecodebench_selfrepair"
              },
              "metrics": [
                {
                  "name": "pass@1",
                  "value": 0.127
                }
              ],
              "tags": [
                "Code fixing"
              ]
            }
          ]
        }
      ],
      "model_limits": {
        "max_sequence_length": 128000
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-05-09"
        }
      ],
      "versions": [
        {
          "version": "1.1.1",
          "available_date": "2024-10-28"
        },
        {
          "version": "1.1.0",
          "available_date": "2024-09-03"
        },
        {
          "version": "1.0.0",
          "available_date": "2024-05-09"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 8,
        "balance_mode": 2
      }
    },
    {
      "model_id": "ibm/granite-embedding-278m-multilingual",
      "label": "granite-embedding-278m-multilingual",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "autoai_rag"
        },
        {
          "id": "embedding"
        }
      ],
      "short_description": "Granite-Embedding-278M-Multilingual is a 278M parameter model from the Granite Embeddings suite that can be used to generate high quality text embeddings.",
      "long_description": "Granite-Embedding-278M-Multilingual is a 278M parameter model from the Granite Embeddings suite that can be used to generate high quality text embeddings. This model produces embedding vectors of size 768 and is trained using a combination of open source relevance-pair datasets with permissive, enterprise-friendly license, and IBM collected and generated datasets. It supports 12 languages:  English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese.",
      "input_tier": "class_c1",
      "output_tier": "class_c1",
      "number_params": "278m",
      "model_limits": {
        "max_sequence_length": 512
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2025-01-15"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 16,
        "balance_mode": 2
      }
    },
    {
      "model_id": "ibm/granite-guardian-3-8b",
      "label": "granite-guardian-3-8b",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.",
      "long_description": "Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.",
      "terms_url": "https://www.ibm.com/support/customer/csol/terms/?id=i126-6883&lc=en",
      "input_tier": "class_12",
      "output_tier": "class_12",
      "number_params": "8b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "classification",
        "generation",
        "extraction"
      ],
      "tasks": [
        {
          "id": "question_answering"
        },
        {
          "id": "summarization"
        },
        {
          "id": "classification"
        },
        {
          "id": "generation"
        },
        {
          "id": "extraction"
        }
      ],
      "model_limits": {
        "max_sequence_length": 8192
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-10-21"
        }
      ],
      "versions": [
        {
          "version": "1.0.0",
          "available_date": "2024-10-21"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 8,
        "balance_mode": 2
      }
    },
    {
      "model_id": "ibm/granite-ttm-1024-96-r2",
      "label": "granite-ttm-1024-96-r2",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "time_series_forecast"
        }
      ],
      "short_description": "TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research",
      "long_description": "TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. Given the last 1024 time-points (i.e. context length), this model can forecast up to next 96 time-points (i.e. forecast length) in future. This model is targeted towards a forecasting setting of context length 1024 and forecast length 96 and recommended for hourly and minutely resolutions (Ex. 10 min, 15 min, 1 hour, etc)",
      "input_tier": "class_14",
      "output_tier": "class_15",
      "number_params": "805k",
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-11-21"
        }
      ],
      "versions": [
        {
          "version": "1.1.0",
          "available_date": "2025-04-25"
        },
        {
          "version": "1.0.0",
          "available_date": "2024-11-21"
        }
      ]
    },
    {
      "model_id": "ibm/granite-ttm-1536-96-r2",
      "label": "granite-ttm-1536-96-r2",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "time_series_forecast"
        }
      ],
      "short_description": "TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research",
      "long_description": "TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. Given the last 1536 time-points (i.e. context length), this model can forecast up to next 96 time-points (i.e. forecast length) in future. This model is targeted towards a forecasting setting of context length 1536 and forecast length 96 and recommended for hourly and minutely resolutions (Ex. 10 min, 15 min, 1 hour, etc)",
      "input_tier": "class_14",
      "output_tier": "class_15",
      "number_params": "805k",
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-11-21"
        }
      ],
      "versions": [
        {
          "version": "1.1.0",
          "available_date": "2025-04-25"
        },
        {
          "version": "1.0.0",
          "available_date": "2024-11-21"
        }
      ]
    },
    {
      "model_id": "ibm/granite-ttm-512-96-r2",
      "label": "granite-ttm-512-96-r2",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "time_series_forecast"
        }
      ],
      "short_description": "TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research",
      "long_description": "TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. Given the last 512 time-points (i.e. context length), this model can forecast up to next 96 time-points (i.e. forecast length) in future. This model is targeted towards a forecasting setting of context length 512 and forecast length 96 and recommended for hourly and minutely resolutions (Ex. 10 min, 15 min, 1 hour, etc)",
      "input_tier": "class_14",
      "output_tier": "class_15",
      "number_params": "805k",
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-11-21"
        }
      ],
      "versions": [
        {
          "version": "1.1.0",
          "available_date": "2025-04-25"
        },
        {
          "version": "1.0.0",
          "available_date": "2024-11-21"
        }
      ]
    },
    {
      "model_id": "ibm/slate-125m-english-rtrvr-v2",
      "label": "slate-125m-english-rtrvr-v2",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "autoai_rag"
        },
        {
          "id": "embedding"
        },
        {
          "id": "rerank"
        },
        {
          "id": "similarity"
        }
      ],
      "short_description": "An embedding model with 512 token limit. It has 125 million parameters and an embedding dimension of 768.",
      "long_description": "This model follows the standard 'sentence transformers' approach, relying on bi-encoders. It generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between two text pieces: text A (query text) and text B (passage text). This process yields sentence embeddings q and p, allowing for comparison through cosine similarity.",
      "input_tier": "class_c1",
      "output_tier": "class_c1",
      "number_params": "125m",
      "model_limits": {
        "max_sequence_length": 512
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 16,
        "balance_mode": 2
      }
    },
    {
      "model_id": "ibm/slate-30m-english-rtrvr-v2",
      "label": "slate-30m-english-rtrvr-v2",
      "provider": "IBM",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "embedding"
        },
        {
          "id": "rerank"
        },
        {
          "id": "similarity"
        }
      ],
      "short_description": "An embedding model with 512 token limit. It has 30 million parameters and an embedding dimension of 384.",
      "long_description": "This model follows the standard 'sentence transformers' approach, relying on bi-encoders. It generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between two text pieces: text A (query text) and text B (passage text). This process yields sentence embeddings q and p, allowing for comparison through cosine similarity.",
      "input_tier": "class_c1",
      "output_tier": "class_c1",
      "number_params": "30m",
      "model_limits": {
        "max_sequence_length": 512
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-08-15"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 16,
        "balance_mode": 2
      }
    },
    {
      "model_id": "intfloat/multilingual-e5-large",
      "label": "multilingual-e5-large",
      "provider": "intfloat",
      "source": "intfloat",
      "indemnity": "NON_IBM",
      "functions": [
        {
          "id": "autoai_rag"
        },
        {
          "id": "embedding"
        },
        {
          "id": "multilingual"
        },
        {
          "id": "rerank"
        },
        {
          "id": "similarity"
        }
      ],
      "short_description": "An embedding model. It has 560 million parameters, has 24 layers and the embedding size is 1024.",
      "long_description": "This model gets continually trained on a mixture of multilingual datasets. It supports 100 languages from xlm-roberta.",
      "input_tier": "class_c1",
      "output_tier": "class_c1",
      "number_params": "560m",
      "model_limits": {
        "max_sequence_length": 512
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-05-16"
        }
      ],
      "supported_languages": [
        "af",
        "am",
        "ar",
        "as",
        "az",
        "be",
        "bg",
        "bn",
        "br",
        "bs",
        "ca",
        "cs",
        "cy",
        "da",
        "de",
        "el",
        "en",
        "eo",
        "es",
        "et",
        "eu",
        "fa",
        "fi",
        "fr",
        "fy",
        "ga",
        "gd",
        "gl",
        "gu",
        "ha",
        "he",
        "hi",
        "hr",
        "hu",
        "hy",
        "id",
        "is",
        "it",
        "ja",
        "jv",
        "ka",
        "kk",
        "km",
        "kn",
        "ko",
        "ku",
        "ky",
        "la",
        "lo",
        "lt",
        "lv",
        "mg",
        "mk",
        "ml",
        "mn",
        "mr",
        "ms",
        "my",
        "ne",
        "nl",
        "no",
        "om",
        "or",
        "pa",
        "pl",
        "ps",
        "pt",
        "ro",
        "ru",
        "sa",
        "sd",
        "si",
        "sk",
        "sl",
        "so",
        "sq",
        "sr",
        "su",
        "sv",
        "sw",
        "ta",
        "te",
        "th",
        "tl",
        "tr",
        "ug",
        "uk",
        "ur",
        "uz",
        "vi",
        "xh",
        "yi",
        "zh"
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 16,
        "balance_mode": 2
      }
    },
    {
      "model_id": "meta-llama/llama-3-1-70b-gptq",
      "label": "llama-3-1-70b-gptq",
      "provider": "Meta",
      "source": "IBM",
      "indemnity": "IBM_COVERED",
      "functions": [
        {
          "id": "base_foundation_model_deployable"
        },
        {
          "id": "lora_fine_tune_trainable"
        }
      ],
      "short_description": "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
      "long_description": "The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
      "terms_url": "https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct/blob/main/LICENSE",
      "input_tier": "",
      "output_tier": "",
      "number_params": "70b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation"
      ],
      "tasks": [
        {
          "id": "question_answering",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "summarization",
          "ratings": {
            "quality": 3
          }
        },
        {
          "id": "classification",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "generation"
        },
        {
          "id": "code"
        },
        {
          "id": "extraction",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "translation"
        }
      ],
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2025-07-04"
        }
      ],
      "versions": [
        {
          "version": "3.1.0",
          "available_date": "2025-07-04"
        }
      ],
      "lora_fine_tuning_parameters": {
        "num_epochs": {
          "default": 10,
          "min": 1,
          "max": 50
        },
        "verbalizer": {
          "default": "### Input: {{input}} \n\n### Response: {{output}}"
        },
        "batch_size": {
          "default": 5,
          "min": 1,
          "max": 16
        },
        "accumulate_steps": {
          "default": 1,
          "min": 1,
          "max": 128
        },
        "learning_rate": {
          "default": 0.00001,
          "min": 0.00001,
          "max": 0.5
        },
        "max_seq_length": {
          "default": 1024,
          "min": 1,
          "max": 8192
        },
        "tokenizer": {
          "default": "meta-llama/llama-3-1-70b-gptq"
        },
        "response_template": {
          "default": "\n### Response:"
        },
        "num_gpus": {
          "default": 1
        },
        "peft_parameters": {
          "type": {
            "supported": [
              "qlora"
            ],
            "default": "qlora"
          },
          "rank": {
            "supported": [
              8,
              16,
              32,
              64,
              128,
              256
            ],
            "default": 8
          },
          "target_modules": {
            "supported": [
              "all-linear",
              "o_proj",
              "v_proj",
              "k_proj",
              "q_proj",
              "up_proj",
              "down_proj",
              "gate_proj"
            ],
            "default": [
              "v_proj",
              "q_proj"
            ]
          },
          "lora_alpha": {
            "default": 32,
            "min": 0
          },
          "lora_dropout": {
            "default": 0.05,
            "min": 0,
            "max": 1
          }
        },
        "gradient_checkpointing": {
          "default": true
        }
      },
      "architecture_type": "llama",
      "curated_model_info": {
        "base_model_id": "meta-llama/llama-3-1-70b-gptq",
        "lora_hardware_spec": "WXaaS-M-Lora",
        "additional_params": "TOOL_CALL_PARSER=llama3_json,VLLM_USE_V1=1,HOME=/home/vllm,OMP_NUM_THREADS=2,MAX_SEQUENCE_LENGTH=128000,MAX_LORAS=8,MAX_CPU_LORAS=10,MAX_LORA_RANK=256,DTYPE=float16",
        "gpu_requirements": {
          "min_count": {
            "a100": 4,
            "h100": 4
          }
        }
      },
      "deployment_parameters": [
        {
          "name": "enable_lora",
          "display_name": "Enable Lora",
          "default": false,
          "type": "boolean"
        }
      ]
    },
    {
      "model_id": "meta-llama/llama-3-1-8b",
      "label": "llama-3-1-8b",
      "provider": "Meta",
      "source": "Hugging Face",
      "indemnity": "NON_IBM",
      "functions": [
        {
          "id": "base_foundation_model_deployable"
        },
        {
          "id": "lora_fine_tune_trainable"
        }
      ],
      "short_description": "Llama-3-1-8b is an auto-regressive language model that uses an optimized transformer architecture.",
      "long_description": "Llama-3-1-8b is a pretrained and fine-tuned generative text model with 8 billion parameters, optimized for multilingual dialogue use cases and code output.",
      "terms_url": "https://www.llama.com/llama3_1/license/",
      "input_tier": "",
      "output_tier": "",
      "number_params": "8b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction"
      ],
      "tasks": [
        {
          "id": "question_answering",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "summarization",
          "ratings": {
            "quality": 3
          }
        },
        {
          "id": "retrieval_augmented_generation",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "classification",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "generation"
        },
        {
          "id": "code"
        },
        {
          "id": "extraction",
          "ratings": {
            "quality": 4
          }
        }
      ],
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-11-25"
        }
      ],
      "versions": [
        {
          "version": "3.1.0",
          "available_date": "2024-08-01"
        }
      ],
      "lora_fine_tuning_parameters": {
        "num_epochs": {
          "default": 5,
          "min": 1,
          "max": 50
        },
        "verbalizer": {
          "default": "### Input: {{input}} \n\n### Response: {{output}}"
        },
        "batch_size": {
          "default": 8,
          "min": 1,
          "max": 16
        },
        "accumulate_steps": {
          "default": 1,
          "min": 1,
          "max": 128
        },
        "learning_rate": {
          "default": 0.00001,
          "min": 0.00001,
          "max": 0.5
        },
        "max_seq_length": {
          "default": 1024,
          "min": 1,
          "max": 8192
        },
        "tokenizer": {
          "default": "meta-llama/llama-3-1-8b"
        },
        "response_template": {
          "default": "\n### Response:"
        },
        "num_gpus": {
          "default": 1
        },
        "peft_parameters": {
          "type": {
            "supported": [
              "lora"
            ],
            "default": "lora"
          },
          "rank": {
            "supported": [
              8,
              16,
              32,
              64,
              128,
              256
            ],
            "default": 32
          },
          "target_modules": {
            "supported": [
              "all-linear",
              "o_proj",
              "v_proj",
              "k_proj",
              "q_proj",
              "up_proj",
              "down_proj",
              "gate_proj"
            ],
            "default": [
              "v_proj",
              "q_proj"
            ]
          },
          "lora_alpha": {
            "default": 64,
            "min": 0,
            "max": 999999
          },
          "lora_dropout": {
            "default": 0.05,
            "min": 0,
            "max": 1
          }
        },
        "gradient_checkpointing": {
          "default": true
        }
      },
      "architecture_type": "llama",
      "curated_model_info": {
        "base_model_id": "meta-llama/llama-3-1-8b",
        "lora_hardware_spec": "WXaaS-S-Lora",
        "additional_params": "VLLM_USE_V1=1,HOME=/home/vllm,OMP_NUM_THREADS=2,MAX_SEQUENCE_LENGTH=131072,MAX_LORAS=16,MAX_CPU_LORAS=18,MAX_LORA_RANK=256,DTYPE=bfloat16",
        "gpu_requirements": {
          "min_count": {
            "a100": 1,
            "h100": 1
          },
          "max_count": {
            "a100": 1,
            "h100": 1
          }
        }
      },
      "deployment_parameters": [
        {
          "name": "enable_lora",
          "display_name": "Enable Lora",
          "default": false,
          "type": "boolean"
        }
      ]
    },
    {
      "model_id": "meta-llama/llama-3-2-11b-vision-instruct",
      "label": "llama-3-2-11b-vision-instruct",
      "provider": "Meta",
      "source": "Hugging Face",
      "indemnity": "NON_IBM",
      "functions": [
        {
          "id": "image_chat"
        },
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "Llama-3-2-11b-vision-instruc is an auto-regressive language model that uses an optimized transformer architecture.",
      "long_description": "Llama-3-2-11b-vision-instruc is a pretrained and fine-tuned generative text model with 11 billion parameters, optimized for multilingual dialogue use cases and code output.",
      "terms_url": "https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",
      "input_tier": "class_9",
      "output_tier": "class_9",
      "number_params": "11b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation",
        "function_calling"
      ],
      "tasks": [
        {
          "id": "question_answering",
          "ratings": {
            "quality": 4
          },
          "benchmarks": [
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "FinQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 14
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 53.89
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 77.85
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 75.69
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 43.36
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 50.08
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 78.78
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 73.36
                }
              ],
              "tags": [
                "Knowledge"
              ]
            }
          ]
        },
        {
          "id": "summarization",
          "ratings": {
            "quality": 3
          },
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.",
              "language": "English",
              "dataset": {
                "name": "Summarization"
              },
              "prompt": {},
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 31.72
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset that summarizes US Congressional and California state bills from 1993â€“2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "English",
              "dataset": {
                "name": "BillSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 23.79
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "English",
              "dataset": {
                "name": "TLDR17"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 8.21
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "French",
              "dataset": {
                "name": "MLSUM"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 15.5
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "German",
              "dataset": {
                "name": "MLSUM"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 16.35
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Arabic",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 4.65
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "French",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 20.03
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Japanese",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 18.68
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Korean",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 15.23
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Portuguese",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 17.8
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Spanish",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 14.8
                }
              ],
              "tags": [
                "Summarization"
              ]
            }
          ]
        },
        {
          "id": "retrieval_augmented_generation",
          "ratings": {
            "quality": 4
          },
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.",
              "language": "English",
              "dataset": {
                "name": "RAG"
              },
              "prompt": {},
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 49.32
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "CLAP NQ"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 41.4
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "French",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 83
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "German",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 77.84
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "Portuguese",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 82.27
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "Spanish",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 79.66
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            }
          ]
        },
        {
          "id": "classification",
          "ratings": {
            "quality": 4
          },
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.",
              "language": "English",
              "dataset": {
                "name": "Classification"
              },
              "prompt": {},
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 56.74
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "20 Newsgroups"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 46.07
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "BBQ"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 78.73
                }
              ],
              "tags": [
                "Safety & Bias"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "CFPB"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 75.44
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "HellaSwag"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 50
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "LegalBench"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 49.68
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "MMLU-Pro"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 37.76
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "OpenBookQA"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 80
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 82.81
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 89.84
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 84.38
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 83.59
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 82.03
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 89.06
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 85.94
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 72.2
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 62.98
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 67.84
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 83.13
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 79.15
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 67.23
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 64.25
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 78.23
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 79.84
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 83.95
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 82.79
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 81.97
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 77.87
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 75.21
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 45.38
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 50.24
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 49.39
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 42.75
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 48.34
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 50.8
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 49.94
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 45.9
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 50.38
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 49.9
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 43.37
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 48.66
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 51
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 50.99
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 42.19
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 61.72
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 49.22
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 59.38
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 64.84
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 65.62
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 73.44
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 67.97
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XWinograd"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 61.72
                }
              ],
              "tags": [
                "Reasoning"
              ]
            }
          ]
        },
        {
          "id": "generation",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.",
              "language": "English",
              "dataset": {
                "name": "Generation"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "SacreBLEU",
                  "value": 8.31
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.",
              "language": "English",
              "dataset": {
                "name": "Arena-Hard"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Win rate",
                  "value": 9.55
                }
              ],
              "tags": [
                "Chatbot Ability"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.",
              "language": "English",
              "dataset": {
                "name": "AttaQ 500"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Safety",
                  "value": 79.68
                }
              ],
              "tags": [
                "Safety & Bias"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "French",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 23.17
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "German",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 24.02
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "Portuguese",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 42.59
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "Spanish",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 48.32
                }
              ],
              "tags": [
                "Generation"
              ]
            }
          ]
        },
        {
          "id": "code"
        },
        {
          "id": "extraction",
          "ratings": {
            "quality": 4
          },
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.",
              "language": "English",
              "dataset": {
                "name": "Extraction"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 46.31
                }
              ],
              "tags": [
                "Extraction"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "Universal NER"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 30.89
                }
              ],
              "tags": [
                "Extraction"
              ]
            }
          ]
        },
        {
          "id": "translation",
          "benchmarks": [
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.",
              "language": "English",
              "dataset": {
                "name": "FLoRes-101"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "SacreBLEU",
                  "value": 34.95
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "French",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 91.88
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "German",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 89.65
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Japanese",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 77.53
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Korean",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 48
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Portuguese",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 89.53
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Spanish",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 90.82
                }
              ],
              "tags": [
                "Translation"
              ]
            }
          ]
        },
        {
          "id": "function_calling",
          "ratings": {
            "quality": 4
          }
        }
      ],
      "model_limits": {
        "max_sequence_length": 131072
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-09-25"
        }
      ],
      "versions": [
        {
          "version": "3.2.0",
          "available_date": "2024-09-25"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 16,
        "balance_mode": 2
      }
    },
    {
      "model_id": "meta-llama/llama-3-2-90b-vision-instruct",
      "label": "llama-3-2-90b-vision-instruct",
      "provider": "Meta",
      "source": "Hugging Face",
      "indemnity": "NON_IBM",
      "functions": [
        {
          "id": "image_chat"
        },
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "Llama-3-2-90b-vision-instruct is an auto-regressive language model that uses an optimized transformer architecture.",
      "long_description": "Llama-3-2-90b-vision-instruct is a pretrained and fine-tuned generative text model with 90 billion parameters, optimized for multilingual dialogue use cases and code output.",
      "terms_url": "https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",
      "input_tier": "class_10",
      "output_tier": "class_10",
      "number_params": "90b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation",
        "function_calling"
      ],
      "tasks": [
        {
          "id": "question_answering",
          "ratings": {
            "quality": 4
          },
          "benchmarks": [
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "FinQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 33
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 60.68
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 82.01
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 85.83
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 30.99
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 56.06
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 86.49
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 77.48
                }
              ],
              "tags": [
                "Knowledge"
              ]
            }
          ]
        },
        {
          "id": "summarization",
          "ratings": {
            "quality": 3
          },
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.",
              "language": "English",
              "dataset": {
                "name": "Summarization"
              },
              "prompt": {},
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 32.87
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset that summarizes US Congressional and California state bills from 1993â€“2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "English",
              "dataset": {
                "name": "BillSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 20.81
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "English",
              "dataset": {
                "name": "TLDR17"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 8.96
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "French",
              "dataset": {
                "name": "MLSUM"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 17.22
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "German",
              "dataset": {
                "name": "MLSUM"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 19.08
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Arabic",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 6.27
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "French",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 22.05
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Japanese",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 19.65
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Korean",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 13.38
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Portuguese",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 21.29
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Spanish",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 17.18
                }
              ],
              "tags": [
                "Summarization"
              ]
            }
          ]
        },
        {
          "id": "retrieval_augmented_generation",
          "ratings": {
            "quality": 4
          },
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.",
              "language": "English",
              "dataset": {
                "name": "RAG"
              },
              "prompt": {},
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 56.77
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "CLAP NQ"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 43.99
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "French",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 84.88
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "German",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 77.56
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "Portuguese",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 79.78
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "Spanish",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 76.15
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            }
          ]
        },
        {
          "id": "classification",
          "ratings": {
            "quality": 4
          },
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.",
              "language": "English",
              "dataset": {
                "name": "Classification"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 56.44
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "20 Newsgroups"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 75.13
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "BBQ"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 95.73
                }
              ],
              "tags": [
                "Safety & Bias"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "CFPB"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 85.92
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "HellaSwag"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 54
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "LegalBench"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 67.26
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "MMLU-Pro"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 53.06
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "OpenBookQA"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 94
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 95.31
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 96.88
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 95.31
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 92.97
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 92.97
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 93.75
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 94.53
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 80
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 77.69
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 82.5
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 90.98
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 86.53
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 84.21
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 77.12
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 86.07
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 88.26
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 91.13
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 92
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 87.8
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 85.71
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 88.16
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 64.4
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 64.27
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 63.89
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 57.8
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 67.1
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 65.08
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 64.17
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 64.51
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 64.43
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 63.78
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 57.64
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 67.62
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 65.17
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 63.97
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 63.28
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 69.53
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 75
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 67.97
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 75.78
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 74.22
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 76.56
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 76.56
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XWinograd"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 89.84
                }
              ],
              "tags": [
                "Reasoning"
              ]
            }
          ]
        },
        {
          "id": "generation",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.",
              "language": "English",
              "dataset": {
                "name": "Generation"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "SacreBLEU",
                  "value": 10.27
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.",
              "language": "English",
              "dataset": {
                "name": "Arena-Hard"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Win rate",
                  "value": 26.01
                }
              ],
              "tags": [
                "Chatbot Ability"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.",
              "language": "English",
              "dataset": {
                "name": "AttaQ 500"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Safety",
                  "value": 75.84
                }
              ],
              "tags": [
                "Safety & Bias"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "French",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 26.25
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "German",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 27.21
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "Portuguese",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 45.94
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "Spanish",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 53.54
                }
              ],
              "tags": [
                "Generation"
              ]
            }
          ]
        },
        {
          "id": "code"
        },
        {
          "id": "extraction",
          "ratings": {
            "quality": 4
          },
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.",
              "language": "English",
              "dataset": {
                "name": "Extraction"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 54.11
                }
              ],
              "tags": [
                "Extraction"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "Universal NER"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 41.38
                }
              ],
              "tags": [
                "Extraction"
              ]
            }
          ]
        },
        {
          "id": "translation",
          "benchmarks": [
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.",
              "language": "English",
              "dataset": {
                "name": "FLoRes-101"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "SacreBLEU",
                  "value": 41.01
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "French",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 92.94
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "German",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 91.29
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Japanese",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 81.29
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Korean",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 52
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Portuguese",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 93.29
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Spanish",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 92.12
                }
              ],
              "tags": [
                "Translation"
              ]
            }
          ]
        },
        {
          "id": "function_calling",
          "ratings": {
            "quality": 4
          }
        }
      ],
      "model_limits": {
        "max_sequence_length": 131072
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-09-25"
        },
        {
          "id": "deprecated",
          "start_date": "2025-11-24",
          "alternative_model_ids": [
            "meta-llama/llama-4-maverick-17b-128e-instruct-fp8"
          ]
        },
        {
          "id": "withdrawn",
          "start_date": "2026-03-31",
          "alternative_model_ids": [
            "meta-llama/llama-4-maverick-17b-128e-instruct-fp8"
          ]
        }
      ],
      "versions": [
        {
          "version": "3.2.0",
          "available_date": "2024-09-25"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 16,
        "balance_mode": 2
      }
    },
    {
      "model_id": "meta-llama/llama-3-3-70b-instruct",
      "label": "llama-3-3-70b-instruct",
      "provider": "Meta",
      "source": "Hugging Face",
      "indemnity": "NON_IBM",
      "functions": [
        {
          "id": "autoai_rag"
        },
        {
          "id": "multilingual"
        },
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "This version of Llama-3.3-70b-instruct is also the FP8 quantized version of the original FP16 weights.",
      "long_description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.",
      "terms_url": "https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE",
      "input_tier": "class_13",
      "output_tier": "class_13",
      "number_params": "70b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation",
        "function_calling"
      ],
      "tasks": [
        {
          "id": "question_answering",
          "ratings": {
            "quality": 4
          },
          "benchmarks": [
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "FinQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 25
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 60.62
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 78.39
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 78.49
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 44.79
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 55.57
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 81.03
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 73.72
                }
              ],
              "tags": [
                "Knowledge"
              ]
            }
          ]
        },
        {
          "id": "summarization",
          "ratings": {
            "quality": 3
          },
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.",
              "language": "English",
              "dataset": {
                "name": "Summarization"
              },
              "prompt": {},
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 33.04
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset that summarizes US Congressional and California state bills from 1993â€“2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "English",
              "dataset": {
                "name": "BillSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 29.95
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "English",
              "dataset": {
                "name": "TLDR17"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 9.3
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "French",
              "dataset": {
                "name": "MLSUM"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 15.48
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "German",
              "dataset": {
                "name": "MLSUM"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 16.4
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Arabic",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 8.52
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "French",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 20
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Japanese",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 24.72
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Korean",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 13.99
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Portuguese",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 18.24
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Spanish",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 15.34
                }
              ],
              "tags": [
                "Summarization"
              ]
            }
          ]
        },
        {
          "id": "retrieval_augmented_generation",
          "ratings": {
            "quality": 4
          },
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.",
              "language": "English",
              "dataset": {
                "name": "RAG"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 51.17
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "CLAP NQ"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 41.94
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "French",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 85.1
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "German",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 78.44
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "Portuguese",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 78.54
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "Spanish",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 76.36
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            }
          ]
        },
        {
          "id": "classification",
          "ratings": {
            "quality": 4
          },
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.",
              "language": "English",
              "dataset": {
                "name": "Classification"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 59.62
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "20 Newsgroups"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 70.59
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "BBQ"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 95.73
                }
              ],
              "tags": [
                "Safety & Bias"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "CFPB"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 85
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "HellaSwag"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 55
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "LegalBench"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 57.67
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "MMLU-Pro"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 53.06
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "OpenBookQA"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 91
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 95.31
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 96.88
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 95.31
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 91.41
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 93.75
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 93.75
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 95.31
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 84.08
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 74.38
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 80
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 90.76
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 87.8
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 79.01
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 75.32
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 86.99
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 89.96
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 90.61
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 90.76
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 89.34
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 86.64
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 91.06
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 65.22
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 64.06
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 63.49
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 58.52
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 68.9
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 65.04
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 63.47
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 65.14
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 63.5
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 62.77
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 58.2
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 68.71
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 63.75
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 62.44
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 64.84
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 70.31
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 74.22
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 69.53
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 74.22
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 72.66
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 76.56
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 79.69
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XWinograd"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 89.06
                }
              ],
              "tags": [
                "Reasoning"
              ]
            }
          ]
        },
        {
          "id": "generation",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.",
              "language": "English",
              "dataset": {
                "name": "Generation"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "SacreBLEU",
                  "value": 7.06
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.",
              "language": "English",
              "dataset": {
                "name": "Arena-Hard"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Win rate",
                  "value": 26.38
                }
              ],
              "tags": [
                "Chatbot Ability"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.",
              "language": "English",
              "dataset": {
                "name": "AttaQ 500"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Safety",
                  "value": 92.37
                }
              ],
              "tags": [
                "Safety & Bias"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "French",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 26.78
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "German",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 29.69
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "Portuguese",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 45.04
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "Spanish",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 54.09
                }
              ],
              "tags": [
                "Generation"
              ]
            }
          ]
        },
        {
          "id": "code"
        },
        {
          "id": "extraction",
          "ratings": {
            "quality": 4
          },
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.",
              "language": "English",
              "dataset": {
                "name": "Extraction"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 52.75
                }
              ],
              "tags": [
                "Extraction"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "Universal NER"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 46.55
                }
              ],
              "tags": [
                "Extraction"
              ]
            }
          ]
        },
        {
          "id": "translation",
          "benchmarks": [
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.",
              "language": "English",
              "dataset": {
                "name": "FLoRes-101"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "SacreBLEU",
                  "value": 40.56
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "French",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 93.29
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "German",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 91.53
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Japanese",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 83.06
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Korean",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 54.12
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Portuguese",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 93.88
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Spanish",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 92.35
                }
              ],
              "tags": [
                "Translation"
              ]
            }
          ]
        },
        {
          "id": "function_calling",
          "ratings": {
            "quality": 4
          }
        }
      ],
      "model_limits": {
        "max_sequence_length": 131072
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-12-06"
        }
      ],
      "versions": [
        {
          "version": "3.3.0",
          "available_date": "2024-12-06"
        }
      ],
      "supported_languages": [
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th"
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 16,
        "balance_mode": 2
      }
    },
    {
      "model_id": "meta-llama/llama-3-405b-instruct",
      "label": "llama-3-405b-instruct",
      "provider": "Meta",
      "source": "Meta",
      "indemnity": "NON_IBM",
      "functions": [
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "Llama-3-405b-instruct is Meta's largest open-sourced foundation model to date, with 405 billion parameters, optimized for dialogue use cases",
      "long_description": "Llama-3-405b-instruct is Meta's largest open-sourced foundation model to date, with 405 billion parameters, optimized for dialogue use cases. It's also the largest open-sourced model ever released. It can also be used as a synthetic data generator, post-training data ranking judge, or model teacher/supervisor that can improve specialized capabilities in derivative, more inference friendly models.",
      "terms_url": "https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE",
      "input_tier": "class_3",
      "output_tier": "class_7",
      "number_params": "405b",
      "min_shot_size": 0,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "function_calling"
      ],
      "tasks": [
        {
          "id": "question_answering",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "summarization",
          "ratings": {
            "quality": 3
          }
        },
        {
          "id": "retrieval_augmented_generation",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "classification",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "generation"
        },
        {
          "id": "code"
        },
        {
          "id": "extraction",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "function_calling"
        }
      ],
      "model_limits": {
        "max_sequence_length": 131072
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-07-23"
        },
        {
          "id": "deprecated",
          "start_date": "2025-11-24",
          "alternative_model_ids": [
            "meta-llama/llama-4-maverick-17b-128e-instruct-fp8"
          ]
        },
        {
          "id": "withdrawn",
          "start_date": "2026-03-31",
          "alternative_model_ids": [
            "meta-llama/llama-4-maverick-17b-128e-instruct-fp8"
          ]
        }
      ],
      "versions": [
        {
          "version": "3.1.0",
          "available_date": "2024-07-23"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 24,
        "balance_mode": 2
      }
    },
    {
      "model_id": "meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
      "label": "llama-4-maverick-17b-128e-instruct-fp8",
      "provider": "Meta",
      "source": "Hugging Face",
      "indemnity": "NON_IBM",
      "functions": [
        {
          "id": "autoai_rag"
        },
        {
          "id": "image_chat"
        },
        {
          "id": "multilingual"
        },
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "Llama 4 Maverick, a 17 billion active parameter model with 128 experts.",
      "long_description": "The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.",
      "terms_url": "https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE",
      "input_tier": "class_9",
      "output_tier": "class_16",
      "number_params": "400b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation",
        "function_calling"
      ],
      "tasks": [
        {
          "id": "question_answering",
          "benchmarks": [
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "FinQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 27
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 60.1
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 79.15
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 83.23
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 30.03
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 55.2
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 82.71
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 73.82
                }
              ],
              "tags": [
                "Knowledge"
              ]
            }
          ]
        },
        {
          "id": "summarization",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.",
              "language": "English",
              "dataset": {
                "name": "Summarization"
              },
              "prompt": {},
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 30.62
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset that summarizes US Congressional and California state bills from 1993â€“2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "English",
              "dataset": {
                "name": "BillSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 30.19
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "English",
              "dataset": {
                "name": "TLDR17"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 8.35
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "French",
              "dataset": {
                "name": "MLSUM"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 14.3
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "German",
              "dataset": {
                "name": "MLSUM"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 17.81
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Arabic",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 6.12
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "French",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 17.13
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Japanese",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 22.69
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Korean",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 13.77
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Portuguese",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 19.38
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Spanish",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 14.75
                }
              ],
              "tags": [
                "Summarization"
              ]
            }
          ]
        },
        {
          "id": "retrieval_augmented_generation",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.",
              "language": "English",
              "dataset": {
                "name": "RAG"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 54.46
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "CLAP NQ"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 49.99
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "French",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 83.4
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "German",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 79.51
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "Portuguese",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 78.02
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "Spanish",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 79.52
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            }
          ]
        },
        {
          "id": "classification",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.",
              "language": "English",
              "dataset": {
                "name": "Classification"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 57.13
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "20 Newsgroups"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 64.55
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "BBQ"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 93.45
                }
              ],
              "tags": [
                "Safety & Bias"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "CFPB"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 81.14
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "HellaSwag"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 55
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "LegalBench"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 59.33
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "MMLU-Pro"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 64.29
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "OpenBookQA"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 86
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 96.09
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 96.09
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 96.09
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 92.97
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 91.41
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 94.53
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 94.53
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 81.48
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 79.84
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 83.4
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 90.76
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 89.34
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 85.12
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 78.97
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 84.3
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 87.24
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 90.32
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 92.68
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 89.8
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 86.67
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 86.67
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 72.89
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 64.22
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 67.14
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 55.73
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 67.45
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 67.86
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 66.86
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 71.86
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 62
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 67.94
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 60
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 69.47
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 65.76
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 63.31
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 73.44
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 75.78
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 79.69
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 75.78
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 78.91
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 81.25
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 85.16
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 79.69
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XWinograd"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 89.06
                }
              ],
              "tags": [
                "Reasoning"
              ]
            }
          ]
        },
        {
          "id": "generation",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.",
              "language": "English",
              "dataset": {
                "name": "Generation"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "SacreBLEU",
                  "value": 9.69
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.",
              "language": "English",
              "dataset": {
                "name": "Arena-Hard"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Win rate",
                  "value": 71.51
                }
              ],
              "tags": [
                "Chatbot Ability"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.",
              "language": "English",
              "dataset": {
                "name": "AttaQ 500"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Safety",
                  "value": 92.49
                }
              ],
              "tags": [
                "Safety & Bias"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "French",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 24.73
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "German",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 27.47
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "Portuguese",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 47
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "Spanish",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 49.89
                }
              ],
              "tags": [
                "Generation"
              ]
            }
          ]
        },
        {
          "id": "code"
        },
        {
          "id": "extraction",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.",
              "language": "English",
              "dataset": {
                "name": "Extraction"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 50.05
                }
              ],
              "tags": [
                "Extraction"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "Universal NER"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 40
                }
              ],
              "tags": [
                "Extraction"
              ]
            }
          ]
        },
        {
          "id": "translation",
          "benchmarks": [
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.",
              "language": "English",
              "dataset": {
                "name": "FLoRes-101"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "SacreBLEU",
                  "value": 41.57
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "French",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 93.88
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "German",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 91.29
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Japanese",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 85.76
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Korean",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 56.47
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Portuguese",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 92.24
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Spanish",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 92.71
                }
              ],
              "tags": [
                "Translation"
              ]
            }
          ]
        },
        {
          "id": "function_calling"
        }
      ],
      "model_limits": {
        "max_sequence_length": 131072
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2025-04-06"
        }
      ],
      "versions": [
        {
          "version": "4.0.0",
          "available_date": "2025-04-06"
        }
      ],
      "supported_languages": [
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th"
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 16,
        "balance_mode": 2
      }
    },
    {
      "model_id": "meta-llama/llama-guard-3-11b-vision",
      "label": "llama-guard-3-11b-vision",
      "provider": "Meta",
      "source": "Hugging Face",
      "indemnity": "NON_IBM",
      "functions": [
        {
          "id": "image_chat"
        },
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "Llama-guard-3-11b-vision is an auto-regressive language model that uses an optimized transformer architecture.",
      "long_description": "Llama-guard-3-11b-vision is a pretrained and fine-tuned generative text model with 11 billion parameters, optimized for multilingual dialogue use cases and code output.",
      "terms_url": "https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",
      "input_tier": "class_9",
      "output_tier": "class_9",
      "number_params": "11b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction"
      ],
      "tasks": [
        {
          "id": "question_answering",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "summarization",
          "ratings": {
            "quality": 3
          }
        },
        {
          "id": "retrieval_augmented_generation",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "classification",
          "ratings": {
            "quality": 4
          }
        },
        {
          "id": "generation"
        },
        {
          "id": "code"
        },
        {
          "id": "extraction",
          "ratings": {
            "quality": 4
          }
        }
      ],
      "model_limits": {
        "max_sequence_length": 131072
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-09-25"
        }
      ],
      "versions": [
        {
          "version": "3.2.0",
          "available_date": "2024-09-25"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 21,
        "balance_mode": 2
      }
    },
    {
      "model_id": "mistral-large-2512",
      "label": "mistral-large-2512",
      "provider": "Mistral AI",
      "source": "Mistral",
      "indemnity": "THIRD_PARTY",
      "functions": [
        {
          "id": "text_chat"
        }
      ],
      "short_description": "Mistral Large 3, is a state-of-the-art, open-weight, general-purpose multimodal model with a granular Mixture-of-Experts architecture. It features 41B active parameters and 675B total parameters.",
      "long_description": "Mistral Large 3 is is instruction post-trained and designed for reliability and long-context comprehension. It is engineered for production-grade assistants, retrieval-augmented systems, scientific workloads, and complex enterprise workflows",
      "input_tier": "tech_preview",
      "output_tier": "tech_preview",
      "number_params": "",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation",
        "function_calling"
      ],
      "tasks": [
        {
          "id": "question_answering"
        },
        {
          "id": "summarization"
        },
        {
          "id": "retrieval_augmented_generation"
        },
        {
          "id": "classification"
        },
        {
          "id": "generation"
        },
        {
          "id": "code"
        },
        {
          "id": "extraction"
        },
        {
          "id": "translation"
        },
        {
          "id": "function_calling",
          "ratings": {
            "quality": 4
          }
        }
      ],
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2025-12-01"
        }
      ],
      "versions": [
        {
          "version": "1.0.0",
          "available_date": "2025-12-01"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 20,
        "balance_mode": 2
      }
    },
    {
      "model_id": "mistralai/mistral-medium-2505",
      "label": "mistral-medium-2505",
      "provider": "Mistral AI",
      "source": "Mistral",
      "indemnity": "THIRD_PARTY",
      "functions": [
        {
          "id": "autoai_rag"
        },
        {
          "id": "image_chat"
        },
        {
          "id": "multilingual"
        },
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "Mistral Medium 3 (25.05) is our latest iteration of the Mistral Medium model family.",
      "long_description": "Mistral Medium 3 (25.05) features multimodal capabilities and an extended context length of up to 128k. It can now process and understand visual inputs as well as long documents, further expanding its range of applications.",
      "terms_url": "https://www.ibm.com/support/customer/csol/terms/?id=i126-6883&lc=en",
      "input_tier": "mistral_large_input",
      "output_tier": "mistral_large",
      "number_params": "",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation",
        "function_calling"
      ],
      "tasks": [
        {
          "id": "question_answering",
          "benchmarks": [
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset with over 8,000 QA pairs about finance written by financial experts to evaluate a model's ability to answer finance questions and do numerical reasoning. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "FinQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 40
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 16.31
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 23.28
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 25.46
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 13.07
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 12.97
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 25.21
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 10 K question-and-answer pairs for each of 26 languages (totaling 260 K pairs) from Google Natural Questions dataset to evaluate a model's multilingual question-answering ability. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MKQA"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 22.34
                }
              ],
              "tags": [
                "Knowledge"
              ]
            }
          ]
        },
        {
          "id": "summarization",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Summarization task that summarizes passages. The ROUGE-L score is averaged over 3 datasets of IT dialogs, technical support dialogs, and social media blog.",
              "language": "English",
              "dataset": {
                "name": "Summarization"
              },
              "prompt": {},
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 35.1
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset that summarizes US Congressional and California state bills from 1993â€“2018 to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "English",
              "dataset": {
                "name": "BillSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 27.6
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses preprocessed posts from Reddit to evaluate a model's ability to summarize text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "English",
              "dataset": {
                "name": "TLDR17"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 8.59
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "French",
              "dataset": {
                "name": "MLSUM"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 12.67
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with over 1.5 M article-and-summary pairs from online newspapers in 5 languages (French, German, Spanish, Russian, Turkish) and English newspapers from CNN and Daily Mail to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "German",
              "dataset": {
                "name": "MLSUM"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 11.31
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Arabic",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 4.69
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "French",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 12.8
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Japanese",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 15.87
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Korean",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 11.9
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Portuguese",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 13.59
                }
              ],
              "tags": [
                "Summarization"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with 1.35 M professionally-annotated summaries of BBC news articles in 44 languages to evaluate a model's ability to summarize multilingual text. Metric shows the ROUGE-L score for the generated summary.",
              "language": "Spanish",
              "dataset": {
                "name": "XLSum"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 11.8
                }
              ],
              "tags": [
                "Summarization"
              ]
            }
          ]
        },
        {
          "id": "retrieval_augmented_generation",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Retrieval-augmented generation (RAG) task that generates answers. The ROUGE-L score is averaged over 3 datasets of questions about subjects from various documents.",
              "language": "English",
              "dataset": {
                "name": "RAG"
              },
              "prompt": {
                "number_of_shots": 2
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 59.34
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a long-form question answering dataset to evaluate a model's ability to use grounded passages to answer questions. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "CLAP NQ"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 54.5
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "French",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 85.13
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "German",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 78.9
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "Portuguese",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 77.76
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for evaluating cross-lingual web page ranking capabilities. In this task, the model is provided with a query and a set of candidate web page titles, and it needs to rank the candidates based on their relevance to the query.",
              "language": "Spanish",
              "dataset": {
                "name": "XGLUE.wpr"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "NDCG",
                  "value": 81.42
                }
              ],
              "tags": [
                "Information Retrieval"
              ]
            }
          ]
        },
        {
          "id": "classification",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Classification task that classifies contractual content and interprets sentiment, emotion, and tone. The F1 score is averaged over 5 datasets.",
              "language": "English",
              "dataset": {
                "name": "Classification"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 58.83
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a collection of almost 20,000 newsgroup documents grouped into 20 categories to evaluate a model's ability to classify text. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "20 Newsgroups"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 64.65
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Evaluates a model's ability to recognize statements that contain biased views about people from what are considered protected classes by US English speakers. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "BBQ"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 94.73
                }
              ],
              "tags": [
                "Safety & Bias"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset with complaints from real customers about credit reports, student loans, money transfers, and other financial services to evaluate a model's ability to classify text. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "CFPB"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 83
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of multiple-choice questions sourced from ActivityNet and WikiHow to evaluate a model's ability to do common-sense scenario completion. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "HellaSwag"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 59
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset that consists of 162 tasks that cover various legal texts, structures, and domains to evaluate a model's ability to reason about legal scenarios. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "LegalBench"
              },
              "prompt": {
                "number_of_shots": 1
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 64.15
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses an improved version of the Massive Multitask Language Understanding (MMLU) dataset to evaluate a model's ability to understand challenging tasks. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "MMLU-Pro"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 53.06
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Simulates an open-book exam format to evaluate a model's ability to use multistep reasoning and rich text comprehension to answer multiple-choice questions. Metric measures the accuracy of answers.",
              "language": "English",
              "dataset": {
                "name": "OpenBookQA"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 96
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 95.31
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 96.88
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 94.53
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 92.97
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 94.53
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 96.09
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset with questions, related passages, and multiple-choice answers in 122 languages from FLORES-200 dataset to evaluate a model's multilingual reading-comprehension and question-answering ability. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "Belebele"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 94.53
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 81.78
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 77.64
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 81.2
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 91.13
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 85.11
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 78.45
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MASSIVE"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 62.03
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Arabic",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 86.44
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "French",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 88.24
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "German",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 88.52
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Japanese",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 90.91
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Korean",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 89.17
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Portuguese",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 86.32
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset of over 1M utterances from interactions with Amazon's voice assistant that is localized into 52 languages and annotated with intent and slot type information to evaluate a model's ability to classify multilingual text with English labels. Metric shows the F1 score.",
              "language": "Spanish",
              "dataset": {
                "name": "MASSIVE_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 86.55
                }
              ],
              "tags": [
                "Classification"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 62.24
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 54.29
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 57.98
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 52.14
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 71
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 48.71
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XMMLU"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 34.97
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 69.57
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 60.42
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 65.22
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Japanese",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 56.92
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Korean",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 73.32
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 63.48
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses translations of the Massive Multitask Language Understanding (MMLU) English dataset, which consists of general knowledge multiple-choice questions to evaluate the model's ability to answer multilingual questions about elementary mathematics, US history, computer science, law, and more. Prompts are in English. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XMMLU_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 60.28
                }
              ],
              "tags": [
                "Knowledge"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 25.78
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 20.31
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 57.81
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference) dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XNLI"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 9.38
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "Arabic",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 20.31
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "French",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 36.72
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "German",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 43.75
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a subset of data from the MNLI (Multi-Genre Natural Language Inference dataset, which has crowd-sourced sentence pairs that are annotated with textual entailment information, translated into 14 languages to evaluate how well a model can classify multilingual sentences when the prompts are in English. Metric measures the accuracy of answers.",
              "language": "Spanish",
              "dataset": {
                "name": "XNLI_en"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 32.81
                }
              ],
              "tags": [
                "Reasoning"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a multilingual collection of Winograd Schemas, which are pairs of sentences with different meanings due to slight word changes, to evaluate a model's ability to understand context and resolve ambiguity in multilingual text. Metric measures the accuracy of answers.",
              "language": "Portuguese",
              "dataset": {
                "name": "XWinograd"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "Accuracy",
                  "value": 90.62
                }
              ],
              "tags": [
                "Reasoning"
              ]
            }
          ]
        },
        {
          "id": "generation",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Generation task that generates text. The SacreBLEU score is calculated for one dataset of marketing emails.",
              "language": "English",
              "dataset": {
                "name": "Generation"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "SacreBLEU",
                  "value": 7.33
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of 500 user prompts from live data submitted to the crowd-sourcing platform Chatbot Arena to evaluate a model's ability to answer questions. Metric shows the win rate for model answers.",
              "language": "English",
              "dataset": {
                "name": "Arena-Hard"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Win rate",
                  "value": 86.6
                }
              ],
              "tags": [
                "Chatbot Ability"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of questions designed to provoke harmful responses to evaluate whether a model is susceptible to safety vulnerabilities. Metric measures the model's safety.",
              "language": "English",
              "dataset": {
                "name": "AttaQ 500"
              },
              "prompt": {
                "number_of_shots": 0
              },
              "metrics": [
                {
                  "name": "Safety",
                  "value": 92.5
                }
              ],
              "tags": [
                "Safety & Bias"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "French",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 26.01
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "German",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 28.48
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "Portuguese",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 39.55
                }
              ],
              "tags": [
                "Generation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a dataset that is part of the XGLUE benchmark designed for cross-lingual natural language generation (NLG) tasks. In this specific task, the goal is to generate a question based on a given passage. The dataset includes passage-question pairs from Bing, and spans six languages: English, French, German, Spanish, Italian, and Portuguese.",
              "language": "Spanish",
              "dataset": {
                "name": "XGLUE.qg"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "ROUGE-L",
                  "value": 49.7
                }
              ],
              "tags": [
                "Generation"
              ]
            }
          ]
        },
        {
          "id": "code"
        },
        {
          "id": "extraction",
          "benchmarks": [
            {
              "type": "watsonx.ai",
              "name": "Language Understanding",
              "description": "Extraction task that extracts named entities and detects sentiment. The F1 score is averaged over 2 datasets: one dataset with 12 named entities and one dataset with 3 sentiment types.",
              "language": "English",
              "dataset": {
                "name": "Extraction"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 53.51
                }
              ],
              "tags": [
                "Extraction"
              ]
            },
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses 43 datasets from various domains, including biomedical, STEM, programming, social media, law, and finance to evaluate a model's ability to recognize named entities. Metric shows the F1 score.",
              "language": "English",
              "dataset": {
                "name": "Universal NER"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "F1 score",
                  "value": 52.17
                }
              ],
              "tags": [
                "Extraction"
              ]
            }
          ]
        },
        {
          "id": "translation",
          "benchmarks": [
            {
              "type": "academic",
              "name": "bluebench",
              "description": "Uses a dataset of English Wikipedia articles that were translated by professional human translators into 101 languages to evaluate a model's ability to translate text. Metric shows the SacreBLEU score for translation quality.",
              "language": "English",
              "dataset": {
                "name": "FLoRes-101"
              },
              "prompt": {
                "number_of_shots": 5
              },
              "metrics": [
                {
                  "name": "SacreBLEU",
                  "value": 39.55
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "French",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 89.06
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "German",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 90.12
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Japanese",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 82.47
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Korean",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 51.53
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Portuguese",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 90.12
                }
              ],
              "tags": [
                "Translation"
              ]
            },
            {
              "type": "academic",
              "name": "Multilingual language understanding",
              "description": "Uses a small dataset of 850 key English words to evaluate a model's ability to translate English sentences into these languages: French, German, Spanish, Portuguese, Japanese, and Korean. Metric shows the string containment score, which measures the word or character distance between the generated sentence and the reference translation.",
              "language": "Spanish",
              "dataset": {
                "name": "Basic English"
              },
              "prompt": {
                "number_of_shots": 10
              },
              "metrics": [
                {
                  "name": "String Containment",
                  "value": 87.41
                }
              ],
              "tags": [
                "Translation"
              ]
            }
          ]
        },
        {
          "id": "function_calling",
          "ratings": {
            "quality": 4
          }
        }
      ],
      "model_limits": {
        "max_sequence_length": 131072
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2025-05-07"
        }
      ],
      "versions": [
        {
          "version": "1.0.0",
          "available_date": "2025-05-07"
        }
      ],
      "supported_languages": [
        "en",
        "fr",
        "de",
        "it",
        "zh",
        "ja",
        "ko",
        "pt",
        "nl",
        "pl"
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 8,
        "balance_mode": 2
      }
    },
    {
      "model_id": "mistralai/mistral-small-3-1-24b-instruct-2503",
      "label": "mistral-small-3-1-24b-instruct-2503",
      "provider": "Mistral AI",
      "source": "Hugging Face",
      "indemnity": "NON_IBM",
      "functions": [
        {
          "id": "autoai_rag"
        },
        {
          "id": "image_chat"
        },
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "This model is an instruction-finetuned version of: Mistral-Small-3.1-24B-Base-2503.",
      "long_description": "Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.",
      "terms_url": "https://www.apache.org/licenses/LICENSE-2.0",
      "input_tier": "class_c1",
      "output_tier": "class_17",
      "number_params": "24b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "retrieval_augmented_generation",
        "retrieval_augmented_generation",
        "classification",
        "generation",
        "code",
        "extraction",
        "function_calling"
      ],
      "tasks": [
        {
          "id": "question_answering"
        },
        {
          "id": "summarization"
        },
        {
          "id": "retrieval_augmented_generation"
        },
        {
          "id": "classification"
        },
        {
          "id": "generation"
        },
        {
          "id": "code"
        },
        {
          "id": "extraction"
        },
        {
          "id": "function_calling"
        }
      ],
      "model_limits": {
        "max_sequence_length": 128000
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2025-04-22"
        }
      ],
      "versions": [
        {
          "version": "1.0.0",
          "available_date": "2025-04-22"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 8,
        "balance_mode": 2
      }
    },
    {
      "model_id": "openai/gpt-oss-120b",
      "label": "gpt-oss-120b",
      "provider": "OpenAI",
      "source": "Hugging Face",
      "indemnity": "NON_IBM",
      "functions": [
        {
          "id": "text_chat"
        },
        {
          "id": "text_generation"
        }
      ],
      "short_description": "openai/gpt-oss-120b is an OpenAIâ€™s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.",
      "long_description": "openai/gpt-oss-120b is an OpenAIâ€™s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases. It was designed for production, general purpose, high reasoning use cases with 117B parameters with 5.1B active parameters.",
      "terms_url": "https://www.apache.org/licenses/LICENSE-2.0",
      "input_tier": "class_8",
      "output_tier": "class_1",
      "number_params": "120b",
      "min_shot_size": 1,
      "task_ids": [
        "question_answering",
        "summarization",
        "classification",
        "generation",
        "code",
        "extraction",
        "translation",
        "function_calling",
        "code-generation",
        "code-explanation",
        "code-fixing"
      ],
      "tasks": [
        {
          "id": "question_answering"
        },
        {
          "id": "summarization"
        },
        {
          "id": "retrieval_augmented_generation"
        },
        {
          "id": "classification"
        },
        {
          "id": "generation"
        },
        {
          "id": "code"
        },
        {
          "id": "extraction"
        },
        {
          "id": "translation"
        },
        {
          "id": "function_calling"
        }
      ],
      "model_limits": {
        "max_sequence_length": 131072
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2025-08-07"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 8,
        "balance_mode": 2
      }
    },
    {
      "model_id": "sentence-transformers/all-minilm-l6-v2",
      "label": "all-minilm6-v2",
      "provider": "sentence-transformers",
      "source": "sentence-transformers",
      "indemnity": "NON_IBM",
      "functions": [
        {
          "id": "embedding"
        },
        {
          "id": "rerank"
        },
        {
          "id": "similarity"
        }
      ],
      "short_description": "An embedding model with 128 token limit. It has 23 million parameters and an embedding dimension of 384.",
      "long_description": "This is a sentence-transformers model. It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
      "input_tier": "class_c1",
      "output_tier": "class_c1",
      "number_params": "23m",
      "model_limits": {
        "max_sequence_length": 512
      },
      "limits": {
        "19e27818-79cf-4137-ae58-da279f9e9d16": {
          "call_time": "10m0s"
        },
        "3f6acf43-ede8-413a-ac69-f8af3bb0cbfe": {
          "call_time": "5m0s"
        },
        "a3d2f92f-06f9-48d0-b2e6-a7ba2b4e0577": {
          "call_time": "10m0s"
        },
        "d18d88b9-be7a-46ec-be1e-aff14904f1e9": {
          "call_time": "10m0s"
        }
      },
      "lifecycle": [
        {
          "id": "available",
          "start_date": "2024-10-16"
        }
      ],
      "router_info": {
        "version": "1.0.0",
        "build": "1.0.139",
        "build_date": "2026-01-09T20:06:07-0800",
        "max_concurrent_requests": 16,
        "balance_mode": 2
      }
    }
  ],
  "next": null
}